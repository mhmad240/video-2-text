import streamlit as st
from faster_whisper import WhisperModel

@st.cache_resource(show_spinner=False)
def load_whisper_model(model_name, device_info):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ø¹ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø«Ù„Ù‰"""
    try:
        st.info(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} Ø¹Ù„Ù‰ {device_info['device'].upper()}...")
        
        model = WhisperModel(
            model_name,
            device=device_info["device"],
            compute_type=device_info["compute_type"],
            download_root="./models"  # âœ… Ù…Ø¬Ù„Ø¯ Ù…Ø®ØµØµ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬
        )
        
        st.success(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ù„Ù‰ {device_info['device'].upper()}")
        return model
        
    except Exception as e:
        st.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")
        # âœ… Fallback Ø¥Ù„Ù‰ CPU ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
        try:
            st.warning("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¹Ù„Ù‰ CPU ÙƒØ¨Ø¯ÙŠÙ„...")
            model = WhisperModel(
                model_name,
                device="cpu",
                compute_type="int8",
                download_root="./models"
            )
            st.success(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} Ø¹Ù„Ù‰ CPU Ø¨Ù†Ø¬Ø§Ø­")
            return model
        except Exception as fallback_error:
            st.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ CPU Ø£ÙŠØ¶Ø§Ù‹: {str(fallback_error)}")
            return None

def clear_model_cache():
    """Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
    st.cache_resource.clear()
    st.success("âœ… ØªÙ… Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬")