import streamlit as st
from faster_whisper import WhisperModel

@st.cache_resource(show_spinner=False)
def _load_model_internal(model_name, device, compute_type):
    """Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙØ¹Ù„ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù…Ø®Ø²Ù† Ù…Ø¤Ù‚ØªØ§Ù‹)"""
    model = WhisperModel(
        model_name,
        device=device,
        compute_type=compute_type,
        download_root="./models"
    )
    return model

def load_whisper_model(model_name, device_info):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©"""
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ ÙØ±ÙŠØ¯ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
    cache_key = f"{model_name}_{device_info['device']}_{device_info['compute_type']}"
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ session_state
    if 'loaded_models' not in st.session_state:
        st.session_state.loaded_models = {}
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ù…Ù„Ø§Ù‹ Ø¨Ø§Ù„ÙØ¹Ù„ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù„Ø³Ø©
    if cache_key in st.session_state.loaded_models:
        return st.session_state.loaded_models[cache_key]
    
    # Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø­Ù…Ù„ - Ù†Ø¹Ø±Ø¶ Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„
    try:
        # ØªØ±Ø¬Ù…Ø© Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        model_names_ar = {
            'tiny': 'Ø§Ù„ØµØºÙŠØ± Ø¬Ø¯Ø§Ù‹',
            'base': 'Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ',
            'small': 'Ø§Ù„ØµØºÙŠØ±',
            'medium': 'Ø§Ù„Ù…ØªÙˆØ³Ø·',
            'large': 'Ø§Ù„ÙƒØ¨ÙŠØ±'
        }
        model_name_ar = model_names_ar.get(model_name, model_name)
        device_ar = 'ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© (CPU)' if device_info['device'] == 'cpu' else 'ÙƒØ±Øª Ø§Ù„Ø´Ø§Ø´Ø© (GPU)'
        
        with st.spinner(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name_ar} Ø¹Ù„Ù‰ {device_ar}..."):
            model = _load_model_internal(
                model_name,
                device_info["device"],
                device_info["compute_type"]
            )
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ session_state
        st.session_state.loaded_models[cache_key] = model
        
        st.success(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name_ar} Ø¨Ù†Ø¬Ø§Ø­!")
        return model
        
    except Exception as e:
        st.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")
        # Fallback Ø¥Ù„Ù‰ CPU
        try:
            st.warning("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¹Ù„Ù‰ CPU ÙƒØ¨Ø¯ÙŠÙ„...")
            model = _load_model_internal(
                model_name,
                "cpu",
                "int8"
            )
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø¯ÙŠÙ„
            fallback_key = f"{model_name}_cpu_int8"
            st.session_state.loaded_models[fallback_key] = model
            
            st.success(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ CPU Ø¨Ù†Ø¬Ø§Ø­")
            return model
        except Exception as fallback_error:
            st.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ CPU Ø£ÙŠØ¶Ø§Ù‹: {str(fallback_error)}")
            return None

def clear_model_cache():
    """Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
    st.cache_resource.clear()
    if 'loaded_models' in st.session_state:
        st.session_state.loaded_models = {}
    st.success("âœ… ØªÙ… Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬")